{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPaizCdlyw4b3sdrGoBV3wU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivam231129/case_study-mercari-prize-suggestion/blob/main/final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atVuUQmnGSUr"
      },
      "source": [
        "#import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from xgboost import XGBRegressor\n",
        "import dill\n",
        "from nltk.tokenize import word_tokenize\n",
        "import gensim.models\n",
        "\n",
        "from scipy.sparse import hstack\n",
        "from sklearn.metrics import mean_squared_log_error\n",
        "\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "import joblib\n",
        "import gc\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pickle\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "import sklearn\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import Dense,Dropout,Embedding,LSTM\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras import Sequential\n",
        "from keras.initializers import he_normal\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Input\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Input\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import LSTM,Bidirectional\n",
        "from keras.layers.core import Dense, Dropout\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "import keras\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "from keras.callbacks import *\n",
        "from keras.models import load_model\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibTrelmR1EJv",
        "outputId": "c7d18ccb-d094-4b5e-9706-cce1f1f5f374"
      },
      "source": [
        "import tensorflow as tf; print(tf.__version__)\n",
        "import keras; print(keras.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n",
            "2.4.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIFLaM20-Rvo",
        "outputId": "32a692ac-3364-47e4-b33a-1bbf9f8b964d"
      },
      "source": [
        "print('The scikit-learn version is {}.'.format(np.__version__))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The scikit-learn version is 1.19.5.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErPgxyv9PHrp",
        "outputId": "8a0ada06-2b63-4323-f9c9-cb7238f370b3"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNKqPe4Yra5U"
      },
      "source": [
        "vectorizer3=joblib.load(\"/content/drive/My Drive/new/vector3.pkl\")\n",
        "\n",
        "  #global t_1\n",
        "t_1 = joblib.load('/content/drive/My Drive/new/embname.pkl')\n",
        "#t_1.fit_on_texts(name_train)\n",
        "#https://subscription.packtpub.com/book/application_development/9781782167853/1/ch01lvl1sec10/tokenizing-sentences-into-words\n",
        "#global t_2\n",
        "t_2 = joblib.load('/content/drive/My Drive/new/embtext.pkl')\n",
        "model =  load_model('/content/drive/My Drive/new/model7.h5')\n",
        "vectorizer4=joblib.load(\"/content/drive/My Drive/new/vector4.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtyMPrN2PleP"
      },
      "source": [
        "\n",
        "def test(X_test):\n",
        "  start_time=time.time()\n",
        "  '''this function takes x and predict the output that is y_pred'''\n",
        "  \n",
        "  # Ref: AAIC Notebook for Donors' Choose\n",
        "  def decontracted(sent):\n",
        "    \n",
        "      '''\n",
        "      Task:   This Function changes common short forms like can't, won't to can not, will not resp. (Decontraction)\n",
        "              This is done to ensure uniformity in the whole text\n",
        "      Input:  Raw Text\n",
        "      Output: Decontracted Text\n",
        "      '''\n",
        "      sent = re.sub(r\"aren\\'t\", \"are not\", sent)\n",
        "      sent = re.sub(r\"didn\\'t\", \"did not\", sent)\n",
        "      sent = re.sub(r\"can\\'t\", \"can not\", sent)\n",
        "      sent = re.sub(r\"couldn\\'t\", \"could not\", sent)\n",
        "      sent = re.sub(r\"won\\'t\", \"would not\", sent)\n",
        "      sent = re.sub(r\"wouldn\\'t\", \"would not\", sent)\n",
        "      sent = re.sub(r\"haven\\'t\", \"have not\", sent)\n",
        "      sent = re.sub(r\"shouldn\\'t\", \"should not\", sent)\n",
        "      sent = re.sub(r\"doesn\\'t\", \"does not\", sent)\n",
        "      sent = re.sub(r\"don\\'t\", \"do not\", sent)\n",
        "      sent = re.sub(r\"didn\\'t\", \"did not\", sent)\n",
        "      sent = re.sub(r\"mustn\\'t\", \"must not\", sent)\n",
        "      sent = re.sub(r\"needn\\'t\", \"need not\", sent)\n",
        "      \n",
        "      return sent\n",
        "  \n",
        "  \n",
        "  X_test.fillna('', inplace=True)\n",
        "  \n",
        "  X_test['item_description']  = X_test['item_description'].str.replace('^no description yet$', '', regex=True)\n",
        " \n",
        "  X_test['name'] = X_test['name'] + \" \" + X_test['brand_name']\n",
        "  X_test['text'] = X_test['item_description'] + \" \" + X_test['name'] + \" \" + X_test['category_name']\n",
        "\n",
        "\n",
        "  X_test['name'] = X_test['name'].apply(lambda x : decontracted(x))\n",
        "  X_test['text'] = X_test['text'].apply(lambda x : decontracted(x))\n",
        "  print(\"######################################preprocessing_done##################################################\")\n",
        "\n",
        "  \n",
        "\n",
        " \n",
        "    \n",
        "\n",
        "\n",
        "  \n",
        "  valid_shipvec = vectorizer3.transform(X_test['shipping'].values.reshape(-1, 1))\n",
        "\n",
        "  valid_conditionvec = vectorizer4.transform(X_test['item_condition_id'].values.reshape(-1, 1))\n",
        "\n",
        "  X_te = hstack((valid_shipvec, valid_conditionvec)).todense()\n",
        "  \n",
        "  print(\"################################################# categorical feature encoding done #####################################################\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      #again tolenizing \n",
        "  max_length=300\n",
        "  #https://subscription.packtpub.com/book/application_development/9781782167853/1/ch01lvl1sec10/tokenizing-sentences-into-words\n",
        "\n",
        "  #t_2.fit_on_texts(text_train)\n",
        "  encoded_test_desc = t_2.texts_to_sequences(X_test['text'])\n",
        "  padded_test_desc = pad_sequences(encoded_test_desc, maxlen=max_length, padding='post')\n",
        "  encoded_test=t_1.texts_to_sequences(X_test['name'])\n",
        "  padded_test=pad_sequences(encoded_test, maxlen=max_length, padding='post')\n",
        "\n",
        "\n",
        "  print(\"################################################# name and text encoding done #####################################################\")\n",
        "\n",
        "\n",
        "  test_2 = [padded_test,padded_test_desc,X_te]\n",
        "\n",
        "  print(\"%%%%%%%%% using already load model%%%%%%\")\n",
        "  # load model\n",
        "  \n",
        "  # summarize model.\n",
        "  model.summary()\n",
        "\n",
        "  print(\"^^^^ prediction^^^^^^\")\n",
        "  y_pred=model.predict(test_2,verbose=1)\n",
        "  y_pred=np.expm1(y_pred)\n",
        "  print(\"predicted_value is =\",y_pred[0])\n",
        " \n",
        "\n",
        "  end_time=time.time() \n",
        "\n",
        "  print(\"time taken in seconds=\",end_time-start_time)\n",
        "  return \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNIJOFdnMxXq"
      },
      "source": [
        "#loading unseen data\n",
        "df=pd.read_csv('/content/drive/My Drive/new/test_stg2.tsv', sep='\\t')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C42MXdMwPvDb",
        "outputId": "56c63f54-6230-4daa-d921-4a4857fe257d"
      },
      "source": [
        "df_elements = df.sample(n=1) #generating one random point from unseen data\n",
        "test(df_elements)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "######################################preprocessing_done##################################################\n",
            "################################################# categorical feature encoding done #####################################################\n",
            "################################################# name and text encoding done #####################################################\n",
            "%%%%%%%%% using already load model%%%%%%\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "desc (InputLayer)               [(None, 300)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "name (InputLayer)               [(None, 300)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 300, 300)     73155600    desc[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 300, 300)     33576900    name[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   (None, 300, 20)      25680       embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 300, 20)      25680       embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_4 (LSTM)                   (None, 300, 20)      3280        lstm_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block_1 (Conv1D)                (None, 298, 32)      1952        lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "block_3 (Conv1D)                (None, 298, 32)      1952        lstm_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 298, 10)      1720        block_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_5 (LSTM)                   (None, 298, 20)      4240        block_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "other (InputLayer)              [(None, 7)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "block_2 (Conv1D)                (None, 296, 16)      496         lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block_4 (Conv1D)                (None, 296, 16)      976         lstm_5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 256)          2048        other[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   (None, 296, 10)      1080        block_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_6 (LSTM)                   (None, 296, 20)      2960        block_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 64)           16448       dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 2960)         0           lstm_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 5920)         0           lstm_6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 64)           4160        dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 8944)         0           flatten[0][0]                    \n",
            "                                                                 flatten_1[0][0]                  \n",
            "                                                                 dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 256)          2289920     concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 32)           8224        dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 64)           2112        dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 1)            65          dense_5[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 109,125,493\n",
            "Trainable params: 2,392,993\n",
            "Non-trainable params: 106,732,500\n",
            "__________________________________________________________________________________________________\n",
            "^^^^ prediction^^^^^^\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "predicted_value is = [15.603582]\n",
            "time taken in seconds= 2.5096828937530518\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oz3yER1UPvUZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KL0GfpI4PvjJ"
      },
      "source": [
        "data_new = pd.read_csv(\"/content/drive/My Drive/new/data_new.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tN1c1wHdZjxg"
      },
      "source": [
        "Y=  np.log1p(data_new['price'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAdlCx4zRPnD"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def final2(X_test,y):\n",
        "  '''this function take x,y and predict the evaluation matrix rmsle'''\n",
        "  start_time=time.time()\n",
        "  \n",
        "  \n",
        "  # Ref: AAIC Notebook for Donors' Choose\n",
        "  def decontracted(sent):\n",
        "    \n",
        "      '''\n",
        "      Task:   This Function changes common short forms like can't, won't to can not, will not resp. (Decontraction)\n",
        "              This is done to ensure uniformity in the whole text\n",
        "      Input:  Raw Text\n",
        "      Output: Decontracted Text\n",
        "      '''\n",
        "      sent = re.sub(r\"aren\\'t\", \"are not\", sent)\n",
        "      sent = re.sub(r\"didn\\'t\", \"did not\", sent)\n",
        "      sent = re.sub(r\"can\\'t\", \"can not\", sent)\n",
        "      sent = re.sub(r\"couldn\\'t\", \"could not\", sent)\n",
        "      sent = re.sub(r\"won\\'t\", \"would not\", sent)\n",
        "      sent = re.sub(r\"wouldn\\'t\", \"would not\", sent)\n",
        "      sent = re.sub(r\"haven\\'t\", \"have not\", sent)\n",
        "      sent = re.sub(r\"shouldn\\'t\", \"should not\", sent)\n",
        "      sent = re.sub(r\"doesn\\'t\", \"does not\", sent)\n",
        "      sent = re.sub(r\"don\\'t\", \"do not\", sent)\n",
        "      sent = re.sub(r\"didn\\'t\", \"did not\", sent)\n",
        "      sent = re.sub(r\"mustn\\'t\", \"must not\", sent)\n",
        "      sent = re.sub(r\"needn\\'t\", \"need not\", sent)\n",
        "      \n",
        "      return sent\n",
        "  \n",
        "  \n",
        "  X_test.fillna('', inplace=True)\n",
        "\n",
        "  X_test['item_description']  = X_test['item_description'].str.replace('^no description yet$', '', regex=True)\n",
        " \n",
        "  X_test['name'] = X_test['name'] + \" \" + X_test['brand_name']\n",
        "  X_test['text'] = X_test['item_description'] + \" \" + X_test['name'] + \" \" + X_test['category_name']\n",
        "\n",
        "\n",
        "  X_test['name'] = X_test['name'].apply(lambda x : decontracted(x))\n",
        "  X_test['text'] = X_test['text'].apply(lambda x : decontracted(x))\n",
        "  print(\"######################################preprocessing_done##################################################\")\n",
        "\n",
        "\n",
        " \n",
        "    \n",
        "\n",
        "\n",
        "  \n",
        "  valid_shipvec = vectorizer3.transform(X_test['shipping'].values.reshape(-1, 1))\n",
        "\n",
        "  valid_conditionvec = vectorizer4.transform(X_test['item_condition_id'].values.reshape(-1, 1))\n",
        "\n",
        "  X_te = hstack((valid_shipvec, valid_conditionvec)).todense()\n",
        "  \n",
        "  print(\"################################################# categorical feature encoding done #####################################################\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      #again tolenizing \n",
        "  max_length=300\n",
        "  #https://subscription.packtpub.com/book/application_development/9781782167853/1/ch01lvl1sec10/tokenizing-sentences-into-words\n",
        "\n",
        "  #t_2.fit_on_texts(text_train)\n",
        "  encoded_test_desc = t_2.texts_to_sequences(X_test['text'])\n",
        "  padded_test_desc = pad_sequences(encoded_test_desc, maxlen=max_length, padding='post')\n",
        "  encoded_test=t_1.texts_to_sequences(X_test['name'])\n",
        "  padded_test=pad_sequences(encoded_test, maxlen=max_length, padding='post')\n",
        "\n",
        "\n",
        "  print(\"################################################# name and text encoding done #####################################################\")\n",
        "\n",
        "\n",
        "  test_2 = [padded_test,padded_test_desc,X_te]\n",
        "\n",
        "  print(\"%%%%%%%%% using already load model%%%%%%\")\n",
        "  # load model\n",
        "  \n",
        "  # summarize model.\n",
        "  model.summary()\n",
        "\n",
        "  print(\"^^^^ prediction^^^^^^\")\n",
        "  y_pred=model.predict(test_2,verbose=1)\n",
        "  print(type(y_pred))\n",
        "\n",
        "  print(\"rmsle=\",np.sqrt(mean_squared_error(np.array(y), y_pred)))\n",
        "\n",
        "\n",
        "\n",
        "  y_pred=np.expm1(y_pred)\n",
        "  print(\"predicted_value is =\",y_pred[0])\n",
        " \n",
        "\n",
        "  end_time=time.time() \n",
        "\n",
        "  print(\"time taken in seconds=\",end_time-start_time)\n",
        "  return \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlnguAIg1ed2",
        "outputId": "807ca23b-9159-4246-9526-b87dec9e8f2f"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0          2.397895\n",
              "1          3.970292\n",
              "2          2.397895\n",
              "3          3.583519\n",
              "4          3.806662\n",
              "             ...   \n",
              "1481656    3.044522\n",
              "1481657    2.708050\n",
              "1481658    2.564949\n",
              "1481659    3.828641\n",
              "1481660    3.135494\n",
              "Name: price, Length: 1481661, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKej5lR0lmHT",
        "outputId": "5219986a-f03d-48fd-9f3b-2ffb89cf62b6"
      },
      "source": [
        "\n",
        "final2(data_new,Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "######################################preprocessing_done##################################################\n",
            "################################################# categorical feature encoding done #####################################################\n",
            "################################################# name and text encoding done #####################################################\n",
            "%%%%%%%%% using already load model%%%%%%\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "desc (InputLayer)               [(None, 300)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "name (InputLayer)               [(None, 300)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 300, 300)     73155600    desc[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 300, 300)     33576900    name[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   (None, 300, 20)      25680       embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 300, 20)      25680       embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_4 (LSTM)                   (None, 300, 20)      3280        lstm_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block_1 (Conv1D)                (None, 298, 32)      1952        lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "block_3 (Conv1D)                (None, 298, 32)      1952        lstm_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 298, 10)      1720        block_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_5 (LSTM)                   (None, 298, 20)      4240        block_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "other (InputLayer)              [(None, 7)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "block_2 (Conv1D)                (None, 296, 16)      496         lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block_4 (Conv1D)                (None, 296, 16)      976         lstm_5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 256)          2048        other[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   (None, 296, 10)      1080        block_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_6 (LSTM)                   (None, 296, 20)      2960        block_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 64)           16448       dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 2960)         0           lstm_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 5920)         0           lstm_6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 64)           4160        dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 8944)         0           flatten[0][0]                    \n",
            "                                                                 flatten_1[0][0]                  \n",
            "                                                                 dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 256)          2289920     concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 32)           8224        dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 64)           2112        dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 1)            65          dense_5[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 109,125,493\n",
            "Trainable params: 2,392,993\n",
            "Non-trainable params: 106,732,500\n",
            "__________________________________________________________________________________________________\n",
            "^^^^ prediction^^^^^^\n",
            "46302/46302 [==============================] - 2031s 44ms/step\n",
            "<class 'numpy.ndarray'>\n",
            "rmsle= 0.4323778407967876\n",
            "predicted_value is = [7.6056223]\n",
            "time taken in seconds= 2139.6443181037903\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gA3GL5SXmmHv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N23FRDVo_RN3"
      },
      "source": [
        "# **given all the train data points at a time the rmsle value is 0.4323778407967876"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28MNnnI9upZO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2P73uLfZusCm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDU0LpJ2upmJ"
      },
      "source": [
        "# **FLASK deployment code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFJn8aP-_fuK"
      },
      "source": [
        "#// https://www.youtube.com/watch?v=mrExsjcvF4o // aaic course how to deploy using flsk\n",
        "#import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "#from xgboost import XGBRegressor\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "#import gensim.models\n",
        "\n",
        "from scipy.sparse import hstack\n",
        "from sklearn.metrics import mean_squared_log_error\n",
        "\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "import joblib\n",
        "import gc\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pickle\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import Dense,Dropout,Embedding,LSTM\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras import Sequential\n",
        "from keras.initializers import he_normal\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Input\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Input\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import LSTM,Bidirectional\n",
        "from keras.layers.core import Dense, Dropout\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "import keras\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.callbacks import EarlyStopping\n",
        "import math\n",
        "from keras.callbacks import *\n",
        "from keras.models import load_model\n",
        "import time\n",
        "import flask\n",
        "from flask import Flask, jsonify, request\n",
        "\n",
        "\n",
        " # Ref: AAIC Notebook for Donors' Choose\n",
        "def decontracted(sent):\n",
        "    \n",
        "      '''\n",
        "      Task:   This Function changes common short forms like can't, won't to can not, will not resp. (Decontraction)\n",
        "              This is done to ensure uniformity in the whole text\n",
        "      Input:  Raw Text\n",
        "      Output: Decontracted Text\n",
        "      '''\n",
        "      sent = re.sub(r\"aren\\'t\", \"are not\", sent)\n",
        "      sent = re.sub(r\"didn\\'t\", \"did not\", sent)\n",
        "      sent = re.sub(r\"can\\'t\", \"can not\", sent)\n",
        "      sent = re.sub(r\"couldn\\'t\", \"could not\", sent)\n",
        "      sent = re.sub(r\"won\\'t\", \"would not\", sent)\n",
        "      sent = re.sub(r\"wouldn\\'t\", \"would not\", sent)\n",
        "      sent = re.sub(r\"haven\\'t\", \"have not\", sent)\n",
        "      sent = re.sub(r\"shouldn\\'t\", \"should not\", sent)\n",
        "      sent = re.sub(r\"doesn\\'t\", \"does not\", sent)\n",
        "      sent = re.sub(r\"don\\'t\", \"do not\", sent)\n",
        "      sent = re.sub(r\"didn\\'t\", \"did not\", sent)\n",
        "      sent = re.sub(r\"mustn\\'t\", \"must not\", sent)\n",
        "      sent = re.sub(r\"needn\\'t\", \"need not\", sent)\n",
        "      \n",
        "      return sent\n",
        "\n",
        "\n",
        "\n",
        "app = Flask(__name__)\n",
        "  # load model\n",
        "\n",
        "#Initializing global variables\n",
        "configuration = tf.compat.v1.ConfigProto()\n",
        "configuration.gpu_options.allow_growth = True\n",
        "session = tf.compat.v1.Session(config=configuration)\n",
        "\n",
        "number = 0\n",
        "name = []\n",
        "item_condition_id = []\n",
        "category_name = []\n",
        "brand_name = []\n",
        "shipping = []\n",
        "item_description = []\n",
        "## loading all weights\n",
        "model =  load_model('model7.h5')\n",
        "vectorizer3=joblib.load('vector3.pkl')\n",
        "vectorizer4=joblib.load('vector4.pkl')\n",
        "t_1=joblib.load('embname.pkl')\n",
        "t_2=joblib.load('embtext.pkl')\n",
        "columns = ['name', 'item_condition_id', 'brand_name', 'category_name', 'shipping', 'item_description']\n",
        "\n",
        "##\n",
        "@app.route('/')\n",
        "def hello_world():\n",
        "    '''localhost :5000 only'''\n",
        "    return 'Hello World!'\n",
        "\n",
        "\n",
        "@app.route('/index')\n",
        "def index():\n",
        "    '''localhost:5000/index'''\n",
        "    return flask.render_template('index.html')\n",
        "\n",
        "@app.route('/index1', methods=['POST'])\n",
        "def index1():\n",
        "\tglobal number \n",
        "\tnumber = int(request.form['nm'])\n",
        "\treturn flask.render_template('temp1/index.html')\n",
        "\n",
        "    \n",
        "\n",
        "@app.route('/predict',methods = ['POST'])\n",
        "def predict():\n",
        "\n",
        "\n",
        "\n",
        "    '''predict function '''\n",
        "\n",
        "  \n",
        "\n",
        "    global number\n",
        "    global name\n",
        "    global item_condition_id\n",
        "    global category_name\n",
        "    global brand_name\n",
        "    global shipping\n",
        "    global item_description\n",
        "    to_predict_list = request.form.to_dict()\n",
        "    \n",
        "    name.append(to_predict_list['name'])\n",
        "    item_condition_id.append(int(to_predict_list['item_condition_id']))\n",
        "    shipping.append(int(to_predict_list['shipping']))\n",
        "    if to_predict_list['category_name']==\"\":\n",
        "        category_name.append(math.nan)\n",
        "    else:\n",
        "        category_name.append(to_predict_list['category_name'])\n",
        "    if to_predict_list['brand_name']==\"\":\n",
        "        brand_name.append(math.nan)\n",
        "    else:\n",
        "        brand_name.append(to_predict_list['brand_name'])\n",
        "    if to_predict_list['item_description']==\"\":\n",
        "        item_description.append(math.nan)\n",
        "    else:\n",
        "        item_description.append(to_predict_list['item_description'])\n",
        "    number = number-1\n",
        "    if number!=0:\n",
        "        return flask.render_template('temp1/index.html')\n",
        "    else:\n",
        "\n",
        "        start_time = time.time()\n",
        "        X_test =  pd.DataFrame({'name' : name})\n",
        "        X_test['item_condition_id'] = item_condition_id\n",
        "        X_test['category_name'] = category_name\n",
        "        X_test['shipping'] = shipping\n",
        "        X_test['brand_name'] = brand_name\n",
        "        X_test['item_description'] = item_description\n",
        "\t\n",
        "        X_test['brand_name'] = X_test['brand_name'].fillna(\"Unknown Brand.\")\n",
        "        X_test[\"item_description\"] = X_test[\"item_description\"].fillna(value=\"No description yet.\")\n",
        "   \n",
        "        X_test.fillna('', inplace=True)\n",
        "\n",
        "        X_test['item_description']  = X_test['item_description'].str.replace('^no description yet$', '', regex=True)\n",
        "  \t\t#del all_text_no_punc\n",
        "        X_test['category_name'] = X_test['category_name'].fillna(value=\"Unknown Category.\")\n",
        "\n",
        "        X_test['item_description']  = X_test['item_description'].str.replace('^no description yet$', '', regex=True)\n",
        "\n",
        "        X_test['name'] = X_test['name'] + \" \" + X_test['brand_name']\n",
        "        X_test['text'] = X_test['item_description'] + \" \" + X_test['name'] + \" \" + X_test['category_name']\n",
        "\n",
        "        X_test['name'] = X_test['name'].apply(lambda x : decontracted(x))\n",
        "        X_test['text'] = X_test['text'].apply(lambda x : decontracted(x))\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "        valid_shipvec = vectorizer3.transform(X_test['shipping'].values.reshape(-1, 1))\n",
        "\n",
        "        valid_conditionvec = vectorizer4.transform(X_test['item_condition_id'].values.reshape(-1, 1))\n",
        "\n",
        "        X_te = hstack((valid_shipvec, valid_conditionvec)).todense()\n",
        "  \n",
        " # print(\"################################################# categorical feature encoding done #####################################################\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      #again tolenizing \n",
        "        max_length=300\n",
        "\n",
        "\n",
        "        encoded_test_desc = t_2.texts_to_sequences(X_test['text'])\n",
        "        padded_test_desc = pad_sequences(encoded_test_desc, maxlen=max_length, padding='post')\n",
        "        encoded_test=t_1.texts_to_sequences(X_test['name'])\n",
        "        padded_test=pad_sequences(encoded_test, maxlen=max_length, padding='post')\n",
        "\n",
        "\n",
        "  #print(\"################################################# name and text encoding done #####################################################\")\n",
        "\n",
        "\n",
        "        test_2 = [padded_test,padded_test_desc,X_te]\n",
        "\n",
        "\n",
        "\n",
        "  #print(\"^^^^ prediction^^^^^^\")\n",
        "        y_pred=model.predict(test_2)\n",
        "\n",
        " # print(\"predicted_value is =\",y_pred)\n",
        "\n",
        "  #end_time=time.time() \n",
        "\n",
        "  #print(\"time taken in seconds=\",end_time-start_time)\n",
        "\n",
        "\n",
        "\n",
        "        y_pred=np.expm1(y_pred)\n",
        "        y_pred= y_pred[0].tolist() \n",
        "\n",
        "\n",
        "    #output = round(prediction[0], 2)\n",
        "        return jsonify({'prediction_ $':y_pred })\n",
        "\n",
        "@app.route('/predict_api',methods=['POST'])\n",
        "def predict_api():\n",
        "    '''\n",
        "    For direct API calls trought request\n",
        "    '''\n",
        "    to_predict_list = request.get_json(force=True)\n",
        "    name.append(to_predict_list['name'])\n",
        "    item_condition_id.append(int(to_predict_list['item_condition_id']))\n",
        "    shipping.append(int(to_predict_list['shipping']))\n",
        "    if to_predict_list['category_name']==\"\":\n",
        "        category_name.append(math.nan)\n",
        "    else:\n",
        "        category_name.append(to_predict_list['category_name'])\n",
        "    if to_predict_list['brand_name']==\"\":\n",
        "        brand_name.append(math.nan)\n",
        "    else:\n",
        "        brand_name.append(to_predict_list['brand_name'])\n",
        "    if to_predict_list['item_description']==\"\":\n",
        "        item_description.append(math.nan)\n",
        "    else:\n",
        "        item_description.append(to_predict_list['item_description'])\n",
        "    number = number-1\n",
        "    if number!=0:\n",
        "        return flask.render_template('temp1/index.html')\n",
        "    else:\n",
        "\n",
        "   \n",
        "        X_test =  pd.DataFrame({'name' : name})\n",
        "        X_test['item_condition_id'] = item_condition_id\n",
        "        X_test['category_name'] = category_name\n",
        "        X_test['shipping'] = shipping\n",
        "        X_test['brand_name'] = brand_name\n",
        "        X_test['item_description'] = item_description\n",
        "\t\n",
        "        X_test['brand_name'] = X_test['brand_name'].fillna(\"Unknown Brand.\")\n",
        "        X_test[\"item_description\"] = X_test[\"item_description\"].fillna(value=\"No description yet.\")\n",
        "   \n",
        "        X_test.fillna('', inplace=True)\n",
        "\n",
        "        X_test['item_description']  = X_test['item_description'].str.replace('^no description yet$', '', regex=True)\n",
        "  \t\t#del all_text_no_punc\n",
        "        X_test['category_name'] = X_test['category_name'].fillna(value=\"Unknown Category.\")\n",
        "\n",
        "        X_test['item_description']  = X_test['item_description'].str.replace('^no description yet$', '', regex=True)\n",
        "\n",
        "        X_test['name'] = X_test['name'] + \" \" + X_test['brand_name']\n",
        "        X_test['text'] = X_test['item_description'] + \" \" + X_test['name'] + \" \" + X_test['category_name']\n",
        "\n",
        "        X_test['name'] = X_test['name'].apply(lambda x : decontracted(x))\n",
        "        X_test['text'] = X_test['text'].apply(lambda x : decontracted(x))\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "        valid_shipvec = vectorizer3.transform(X_test['shipping'].values.reshape(-1, 1))\n",
        "\n",
        "        valid_conditionvec = vectorizer4.transform(X_test['item_condition_id'].values.reshape(-1, 1))\n",
        "\n",
        "        X_te = hstack((valid_shipvec, valid_conditionvec)).todense()\n",
        "  \n",
        " # print(\"################################################# categorical feature encoding done #####################################################\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      #again tolenizing \n",
        "        max_length=300\n",
        "\n",
        "\n",
        "        encoded_test_desc = t_2.texts_to_sequences(X_test['text'])\n",
        "        padded_test_desc = pad_sequences(encoded_test_desc, maxlen=max_length, padding='post')\n",
        "        encoded_test=t_1.texts_to_sequences(X_test['name'])\n",
        "        padded_test=pad_sequences(encoded_test, maxlen=max_length, padding='post')\n",
        "\n",
        "\n",
        "  #print(\"################################################# name and text encoding done #####################################################\")\n",
        "\n",
        "\n",
        "        test_2 = [padded_test,padded_test_desc,X_te]\n",
        "\n",
        "\n",
        "\n",
        "  #print(\"^^^^ prediction^^^^^^\")\n",
        "        y_pred=model.predict(test_2)\n",
        "\n",
        " # print(\"predicted_value is =\",y_pred)\n",
        "\n",
        "  #end_time=time.time() \n",
        "\n",
        "  #print(\"time taken in seconds=\",end_time-start_time)\n",
        "\n",
        "\n",
        "\n",
        "        y_pred=np.expm1(y_pred)\n",
        "        y_pred= y_pred[0].tolist() \n",
        "\n",
        "\n",
        "    #output = round(prediction[0], 2)\n",
        "    return jsonify(y_pred[0])\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjLmRNVuyVab"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}